{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import sys\n",
    "import warnings\n",
    "import json\n",
    "import flatdict\n",
    "from neo4j_tools import neo4j_connect,  results_2_dict_list\n",
    "#import os \n",
    "#cwd = os.getcwd()\n",
    "#print(cwd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(query):\n",
    "    q = nc.commit_list([query])\n",
    "    if not q:\n",
    "        return False\n",
    "    dc = results_2_dict_list(q)\n",
    "    if not dc:\n",
    "        return False\n",
    "    else:\n",
    "        return dc\n",
    "\n",
    "def query_ind_count(query):\n",
    "    q = nc.commit_list([query])\n",
    "    if not q:\n",
    "        return False\n",
    "    dc = results_2_dict_list(q)\n",
    "    if not dc:\n",
    "        return False\n",
    "    if not ('ind_count' in dc[0].keys()):\n",
    "        warnings.warn(\"Query has no ind_count\")\n",
    "        return False\n",
    "    else:\n",
    "        return dc[0]['ind_count']\n",
    "\n",
    "def runtest(objects, testconfig, out):\n",
    "    for o in objects:\n",
    "        print(\"Testing object \"+o)\n",
    "        testconfig['objectlabel'] = o\n",
    "        description = testconfig['description']\n",
    "        q1 = testconfig['baseline'] % o\n",
    "        q2 = testconfig['test'] % o\n",
    "        out[o][description] = compare(label=o,description=description,query1=q1,query2=q2) \n",
    "    \n",
    "def compare(label, description, query1, query2, verbose = False, write_reports = False):\n",
    "    r1 = query(query1)[0]\n",
    "    r2 = query(query2)[0]\n",
    "    if r1['ind_count'] == r2['ind_count']:\n",
    "        if verbose:\n",
    "            print(query2)\n",
    "            print(\"Testing assertion:\" + description)\n",
    "            print(\"Result: True\")\n",
    "        return True\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"Testing assertion:\" + description)\n",
    "            print(query2)\n",
    "            print(\"Result: inds_in_datset: %d ; Compliant with pattern: %d\" % (r1['ind_count'],  r2['ind_count']))\n",
    "        # Should probably turn this into a report\n",
    "        if write_reports:\n",
    "            bad_inds = list(set(r1['ind_list']) - set(r2['ind_list']))\n",
    "            file = open(label + \".report\", 'w')\n",
    "            file.write(json.dumps(bad_inds))\n",
    "            file.close()\n",
    "        return False\n",
    "\n",
    "def to_label_list(olist):\n",
    "    objects = []\n",
    "    for o in olist:\n",
    "        objects.append(o['label'])\n",
    "    return objects\n",
    "\n",
    "def prepare_test_results(objects):\n",
    "    test_stats = {}\n",
    "    for d in objects:\n",
    "        test_stats[d] = {}\n",
    "    return test_stats\n",
    "\n",
    "def assemble_query(conf,var):\n",
    "    query = \"\"\n",
    "    for queryfragment in conf.split(\"|\"):\n",
    "        query += var[queryfragment]+\" \"\n",
    "    return query\n",
    "\n",
    "def extract_test_parameters(test, var, cypher_return_clause):\n",
    "    testconfig = {}\n",
    "    testconfig['description'] = test['description']\n",
    "    testconfig['baseline'] = assemble_query(test['baseline'],var) + cypher_return_clause\n",
    "    testconfig['test'] =  assemble_query(test['test_base'],var)+test['test']+ cypher_return_clause\n",
    "    return testconfig\n",
    "\n",
    "def run_tests(tests,testset,test_stats):\n",
    "    for test in tests:\n",
    "        print(\"Running test: \"+test['description'])\n",
    "        runtest(objects, extract_test_parameters(test,testset['vars'],testset['query_return']), test_stats)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.load(open('schema/neo_schema.yml'))\n",
    "nc = neo4j_connect(config['url'], config['usr'], config['pwd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsconfig = yaml.load(open('schema/kb_schema.yml'))\n",
    "testsets = testsconfig['testsets']\n",
    "for testset in testsets:\n",
    "    tests = testset['tests']\n",
    "    objects = to_label_list(query(testset['objects']))\n",
    "    test_stats = prepare_test_results(objects)\n",
    "    run_tests(tests,testset,test_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(json.dumps(test_stats, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures = flatdict.FlatDict(test_stats).values()\n",
    "fail = False in failures\n",
    "if fail:\n",
    "    failed = failures.count(False)\n",
    "    print(str(failed) + \" out of \" + str(len(failures))+ \" tests failed.\")\n",
    "else:\n",
    "    print(\"All tests passed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
